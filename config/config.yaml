root: /storage_common/nobilm/diffusion
run_name: try_replicating_log_pos_ddpm_50_smaller_net_higher_tmax_20
wandb_project: diffusion
seed: 7
dataset_seed: 15
device: cuda:3
mixed_precision: false

use_grokfast: true
# use_ema: true
strict_irreps: False

one_hot_scaling_factor: 1 #.25

model_builders:
    - source.model.moreGNNLayers
    - Heads

# bessel basis
r_max: 15.0
num_basis: 16

# hyperparams
latent_dim: 64
l_max: 1

end_of_epoch_callbacks: [source.diffusion_utils.sample_with_diffusion.generate_single_mol]

# --- HEADS --- #
heads:
    noise:
        out_field: noise
        out_irreps: 3x0e # 1x1o could be better

head_wds: 0.0
noise_mlp_latent_dimensions: [64, 32]
noise_mlp_nonlinearity: silu
noise_has_bias: true

# --- D A T A S E T --- #
dataset_list:
  - dataset: npz
    dataset_input: /storage_common/nobilm/pretrain_paper/PCQM4Mv2Dataset/single_mol/single_mol_1000_times
    # inmemory: false
    transforms: [source.transforms.diffusion_transf.center_pos_transf]
    key_mapping:
      coords: pos # key=key in npz: value: key in AtomicData
      atomic_num: node_types

validation_dataset_list:
  - validation_dataset: npz
    validation_dataset_input: /storage_common/nobilm/pretrain_paper/PCQM4Mv2Dataset/single_mol/single_mol_source
    # inmemory: false
    transforms: [source.transforms.diffusion_transf.center_pos_transf]
    key_mapping:
      coords: pos
      atomic_num: node_types

# - register fields - #
# node_fields:
#   - node_types # could be not needed registered by def

# Field Properties
num_types: 22 # embedding_dimensionality of node_types, it must be present even if node_types not used in fwd | in one-hot case it is num_classes
node_attributes: # if fixed (N,) if not fixed (1,N); for coords: same with an additional dim at -1 always equal to 3
  node_types: # this kword must match the red kword in key_mapping
    fixed: true # if equal for each frame, if so they must not have the batch dim in the npz


# --- L O S S --- #
loss_coeffs:
    - noise:
      - MSELoss
        # - source.diffusion_utils.scheduler.DiffusionLoss

# --- M E T R I C S --- #
metrics_components:
    - noise:
      - MSELoss
        # - source.diffusion_utils.scheduler.DiffusionLoss

# --- T R A I N I N G --- #
dataset_num_workers: 40

accumulation_steps: 1
batch_size: 20
train_dloader_n_workers: 10

validation_batch_size: 256
val_dloader_n_workers: 1

max_epochs: 20000
learning_rate: 3.e-4

# learning rate scheduler
warmup_epochs: 1%
lr_scheduler_name: CosineAnnealingLR

early_stopping_patiences:
  validation_loss: 10










###################
default_dtype: float32
append: true # append (bool): if True, append the old model files and append the same logfile
debug: false

# - radial basis - #
edge_radial_attrs_basis: geqtrain.nn.BesselBasisVec
TanhCutoff_n: 6

# - symmetry - #
parity: o3_full

# --- interaction layers --- #
gnn_layers: 2 # this includes the final interaction block
num_layers: 2 # per interaction block
env_embed_multiplicity: 32

two_body_latent: geqtrain.nn.ScalarMLPFunction
two_body_latent_mlp_latent_dimensions: [64]
two_body_latent_mlp_nonlinearity: silu
two_body_latent_has_bias: true

latent: geqtrain.nn.ScalarMLPFunction
latent_mlp_latent_dimensions: [64]
latent_mlp_nonlinearity: silu
latent_has_bias: true

env_embed: geqtrain.nn.ScalarMLPFunction
env_embed_mlp_latent_dimensions: [64]
env_embed_mlp_nonlinearity: silu
env_embed_has_bias: true

# --- update attrs --- #
update_mlp_latent_dimensions: [64]
update_mlp_nonlinearity: silu
update_has_bias: true
update_wd: true

update_emb_mlp_latent_dimensions: [64]
update_emb_mlp_nonlinearity: silu
update_emb_has_bias: true
update_emb_wd: true

update_0_mlp_latent_dimensions: [64]
update_0_mlp_nonlinearity: silu
update_0_has_bias: true
update_0_wd: true

last_interaction_layer_output_ls: [0, 1]

# logging
verbose: info
wandb: true
wandb_watch: true # - log gradients and/or parameters of the model - #
code_folder_name: source # - use if you run geqtrain from a different repo, to save source code of your repo - #
wandb_watch_kwargs:
    # log: 'gradients' # 'gradients', 'parameters', 'all', comment whole wandb_watch_kwargs to do not log any of previous
    log_freq: 1000  # upload log every N batches

# optimizer
optimizer_name: AdamW
optimizer_params:
  amsgrad: false
  betas: !!python/tuple
  - 0.9
  - 0.999
  eps: 1.0e-08 # do not change this
  weight_decay: 0
  fused: true

# Configure maximum batch sizes to avoid GPU memory errors. These parameters have to be configured according to your GPU RAM
skip_chunking: true
batch_max_atoms: 1000000             # Limit the maximum number of nodes of a graph to be loaded on memory in a single batch

dloader_timeout: 0
dloader_prefetch_factor: 4

train_val_split: random
shuffle: true
report_init_validation: false


mlp_latent_dimensions: [64]
mlp_nonlinearity: silu
has_bias: true
wd: true